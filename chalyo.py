# -*- coding: utf-8 -*-
"""chalyo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cfH8tH_cm8DUAFFahjAhqQrTaJgPrwCK
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/dataset/waterquality_1.csv'

# Load your dataset
dataset = pd.read_csv('/content/drive/MyDrive/dataset/waterquality_1.csv')

# Assuming the last column is your target variable (y_train)
y_train = dataset.iloc[:, -1]

# Assuming the rest of the columns are your features (X_train)
X_train = dataset.iloc[:, :-1]

water = pd.read_csv('/content/drive/MyDrive/dataset/waterquality_1.csv')
water.head()
water.describe

water = water.drop(columns=['Station', 'Station Name', 'Year', 'Location 1'])

water = water.rename(columns={
    'Overall WQI' : 'WQI',
    'WQI FC'      : 'Fecal',
    'WQI Oxy'     : 'Oxygen',
    'WQI pH'      : 'pH',
    'WQI TSS'     : 'Tot_Sediment', # Total Suspended Sediment
    'WQI Temp'    : 'Temp',
    'WQI TPN'     : 'Nitrogen',
    'WQI TP'      : 'Phosphorus',
    'WQI Turb'    : 'Turbidity',
})

missing_values = water.isnull().sum()
print("Missing Values:\n", missing_values)

# Canadian Council of Ministers of the Environment Water Quality Index (CCME WQI)
# 95 - 100 Exellent
# 80 - 94  Good
# 60 - 79  Fair
# 45 - 59  Marginal
# 0  - 44  Poor

conditions = [
    (water.WQI >= 95) & (water.WQI <= 100),
    (water.WQI >= 80) & (water.WQI <= 94),
    (water.WQI >= 60) & (water.WQI <= 79),
    (water.WQI >= 45) & (water.WQI <= 59),
    (water.WQI >= 0)  & (water.WQI <= 44),
]

values = [5, 4, 3, 2, 1]

water['Quality'] = np.select(conditions, values)

water = water.drop(columns=['WQI'])
water.head()

y = water['Quality']

X = water.drop(columns=['Quality'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import numpy as np
import pandas as pd

class SimpleDecisionTreeClassifier:
    def __init__(self):
        self.tree = None
        self.feature_importances_ = None

    @staticmethod
    def gini_impurity(y):
        if len(y) == 0:
            return 0.0
        class_probs = np.bincount(y) / len(y)
        gini = 1.0 - np.sum(class_probs ** 2)
        return gini

    def get_gini(self, y):
        return self.gini_impurity(y)

    def best_split(self, X, y):
        features = list(X.columns)
        best_feature, best_value, best_gini = None, None, float('inf')

        for feature in features:
            values = sorted(X[feature].unique())
            for value in values:
                left_mask = X[feature] <= value
                right_mask = ~left_mask

                y_left = y[left_mask]
                y_right = y[right_mask]

                gini_left = self.get_gini(y_left)
                gini_right = self.get_gini(y_right)

                n_left = len(y_left)
                n_right = len(y_right)

                w_left = n_left / (n_left + n_right)
                w_right = n_right / (n_left + n_right)

                w_gini = w_left * gini_left + w_right * gini_right

                if w_gini < best_gini:
                    best_feature, best_value, best_gini = feature, value, w_gini

        return best_feature, best_value

    def grow_tree(self, X, y, depth, min_samples_split):
        if depth == 0 or len(y) < min_samples_split:
            return np.bincount(y).argmax() if len(y) > 0 else None

        best_feature, best_value = self.best_split(X, y)

        if best_feature is None:
            return np.bincount(y).argmax() if len(y) > 0 else None

        left_mask = X[best_feature] <= best_value
        right_mask = ~left_mask

        left_subtree = self.grow_tree(X[left_mask], y[left_mask], depth - 1, min_samples_split)
        right_subtree = self.grow_tree(X[right_mask], y[right_mask], depth - 1, min_samples_split)

        return (best_feature, best_value, left_subtree, right_subtree)

    def fit(self, X, y, max_depth=None, min_samples_split=None):
        self.tree = self.grow_tree(X, y, max_depth, min_samples_split)
        self.feature_importances_ = self.calculate_feature_importances(X, y)

    def calculate_feature_importances(self, X, y):
        gini_values = []
        for feature in X.columns:
            unique_values = X[feature].unique()
            gini_sum = 0
            for value in unique_values:
                value_mask = X[feature] == value
                y_values = y[value_mask]
                gini = self.get_gini(y_values) * len(y_values) / len(y)
                gini_sum += gini
            gini_values.append((feature, gini_sum))
        gini_values.sort(key=lambda x: x[1])
        return gini_values

    def predict_obs(self, x, tree):
        if isinstance(tree, tuple):
            feature, value, left_subtree, right_subtree = tree
            if x[feature] <= value:
                return self.predict_obs(x, left_subtree)
            else:
                return self.predict_obs(x, right_subtree)
        else:
            return tree

    def predict(self, X):
        if self.tree is None:
            raise ValueError("The model has not been trained yet. Call fit() first.")
        predictions = []
        for _, x in X.iterrows():
            prediction = self.predict_obs(x, self.tree)
            predictions.append(prediction)
        return predictions

simple_tree_classifier = SimpleDecisionTreeClassifier()

simple_tree_classifier.fit(X_train, y_train, max_depth=10, min_samples_split=20)

predictions = simple_tree_classifier.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy}')

print('Classification Report:')
print(classification_report(y_test, predictions))

print('Confusion Matrix:')
print(confusion_matrix(y_test, predictions))

plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, predictions), annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Predictions on the training set
y_train_pred = simple_tree_classifier.predict(X_train)

# Predictions on the test set
y_test_pred = simple_tree_classifier.predict(X_test)

# Calculate metrics for the training set
accuracy_train = accuracy_score(y_train, y_train_pred)
precision_train = precision_score(y_train, y_train_pred, average='weighted')
recall_train = recall_score(y_train, y_train_pred, average='weighted')
f1_train = f1_score(y_train, y_train_pred, average='weighted')

# Calculate metrics for the test set
accuracy_test = accuracy_score(y_test, y_test_pred)
precision_test = precision_score(y_test, y_test_pred, average='weighted')
recall_test = recall_score(y_test, y_test_pred, average='weighted')
f1_test = f1_score(y_test, y_test_pred, average='weighted')

# Print the results
print("Metrics for the training set:")
print(f"Accuracy: {accuracy_train:.2f}")
print(f"Precision: {precision_train:.2f}")
print(f"Recall: {recall_train:.2f}")
print(f"F1 Score: {f1_train:.2f}")

print("\nMetrics for the test set:")
print(f"Accuracy: {accuracy_test:.2f}")
print(f"Precision: {precision_test:.2f}")
print(f"Recall: {recall_test:.2f}")
print(f"F1 Score: {f1_test:.2f}")

#random forest
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming the last column is  target variable (y_train)
y_train = water['Quality']

# Assuming the rest of the columns are  features (X_train)
X_train = water.drop(columns=['Quality'])

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

import numpy as np
import pandas as pd
from sklearn.utils import resample

class SimpleRandomForestClassifier:
    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.trees = []

    def bootstrap_sample(self, X, y):
        indices = resample(np.arange(len(X)), replace=True)
        X_sample, y_sample = X.iloc[indices], y.iloc[indices]
        return X_sample, y_sample

    def fit(self, X, y):
        print("Starting Random Forest fitting...")
        for _ in range(self.n_estimators):
            print(f"Fitting tree {_ + 1}...")
            X_sample, y_sample = self.bootstrap_sample(X, y)
            tree = SimpleDecisionTreeClassifier()
            tree.fit(X_sample, y_sample, max_depth=self.max_depth, min_samples_split=self.min_samples_split)
            print(f"Tree {_ + 1} fitted.")
            self.trees.append(tree)
        print("Random Forest fitting complete.")

    def predict(self, X):
        predictions = np.zeros((len(X), len(self.trees)))

        for i, tree in enumerate(self.trees):
            tree_predictions = np.array([tree.predict_obs(x, tree.tree) for _, x in X.iterrows()])
            predictions[:, i] = tree_predictions

        # Print the predictions array
        print("Predictions array:")
        print(predictions)

        # Use majority voting to get the final predictions
        final_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x.astype(int))), axis=1, arr=predictions)
        return final_predictions

# Instantiate the classifier
simple_rf_classifier = SimpleRandomForestClassifier(n_estimators=20, max_depth=10, min_samples_split=25)

# Fit the model on the training data
simple_rf_classifier.fit(X_train, y_train)

# Make predictions on the test data
rf_predictions = simple_rf_classifier.predict(X_test)

# Evaluate the performance of the Random Forest classifier
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f'Random Forest Accuracy: {rf_accuracy:.2f}')

# Display classification report
print('Random Forest Classification Report:')
print(classification_report(y_test, rf_predictions))

# Display confusion matrix
print('Random Forest Confusion Matrix:')
print(confusion_matrix(y_test, rf_predictions))

import pickle
with open('model.pkl', 'wb') as f:
    pickle.dump(rf_predictions, f)

with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

input_data = (93, 86, 96, 84, 90, 100, 84, 87)

# changing the input_data into numpy array
input_data_as_numpy_array = np.asarray(input_data)
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

print(input_data_reshaped)

import pickle
from keras.models import model_from_json


with open('model/model_architecture.pkl', 'rb') as f:
    model_architecture = pickle.load(f)

